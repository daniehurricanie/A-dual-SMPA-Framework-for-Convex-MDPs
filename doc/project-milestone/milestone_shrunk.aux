\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zahavy2021reward}
\citation{zahavy2021reward}
\citation{zahavy2021reward}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Dual--SPMA loop.} Dual ascent chooses $y_k$, which induces a shaped reward for the SPMA policy step; discounted occupancies feed the next dual update.}}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dualspma}{{1}{1}{\textbf {Dual--SPMA loop.} Dual ascent chooses $y_k$, which induces a shaped reward for the SPMA policy step; discounted occupancies feed the next dual update}{figure.caption.3}{}}
\providecommand \oddpage@label [2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Convex MDP as a two-player game (adapted from \citet  {zahavy2021reward}, Fig.~1). The cost player provides non-stationary shaped rewards $r_t = -\lambda ^t$ to the agent, observing the resulting occupancy measures $d_\pi ^t$. From the agent's perspective, this reduces to standard RL with time-varying rewards.}}{1}{figure.caption.5}\protected@file@percent }
\newlabel{fig:convex_mdp_game}{{2}{1}{Convex MDP as a two-player game (adapted from \citet {zahavy2021reward}, Fig.~1). The cost player provides non-stationary shaped rewards $r_t = -\lambda ^t$ to the agent, observing the resulting occupancy measures $d_\pi ^t$. From the agent's perspective, this reduces to standard RL with time-varying rewards}{figure.caption.5}{}}
\citation{zahavy2021reward}
\citation{asad2024fast}
\citation{ding2020npgpd}
\citation{zahavy2021reward}
\citation{asad2024fast}
\citation{ding2020npgpd}
\citation{zahavy2021reward}
\citation{zahavy2021reward}
\bibstyle{abbrvnat}
\bibdata{refs}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Meta-algorithm for Convex MDPs \citep  {zahavy2021reward}}}{2}{algocf.1}\protected@file@percent }
\newlabel{alg:convex_mdp_meta}{{1}{2}{Paper 1: \emph {Reward is Enough for Convex MDPs} (NeurIPS 2021)}{algocf.1}{}}
\bibcite{asad2024fast}{{1}{2024}{{Asad et~al.}}{{Asad, Harikandeh, Laradji, Roux, and Vaswani}}}
\bibcite{ding2020npgpd}{{2}{2020}{{Ding et~al.}}{{Ding, Zhang, Ba{\c {s}}ar, and Jovanovi{\'c}}}}
\bibcite{zahavy2021reward}{{3}{2021}{{Zahavy et~al.}}{{Zahavy, O'Donoghue, Desjardins, and Singh}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Three perspectives that our project unifies or compares against.}}{3}{table.caption.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Instantiations of the convex MDP framework (adapted from \citet  {zahavy2021reward}, Table~1). Different choices of objective $f$ and player algorithms recover well-known RL problems.}}{3}{table.caption.11}\protected@file@percent }
\newlabel{tab:convex_mdp_examples}{{2}{3}{Instantiations of the convex MDP framework (adapted from \citet {zahavy2021reward}, Table~1). Different choices of objective $f$ and player algorithms recover well-known RL problems}{table.caption.11}{}}
\gdef \@abspage@last{3}
